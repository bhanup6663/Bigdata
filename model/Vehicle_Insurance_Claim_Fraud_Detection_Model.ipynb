{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is processed.\n",
      "+-----------+------------------+----+------------+---------+----------+------------+------+---------------------+---------------------+-------+-------------+-----------------+------------+--------------------+------------------------+--------------------+-----------+---------------------+-------------+------------------+-----------------------+--------------------+----------------------------+-------------------------+--------------------------+--------------------+-------------------------+-------------------------+----------------------+-----------------+---------------------------+---------------------------+--------------------+------------------+--------------+-----------------+--------------+--------------------+------------------------+--------------------+-------------+---------------------+-------------+------------------+-----------------------+--------------------+----------------------------+-------------------------+--------------------------+--------------------+-------------------------+-------------------------+----------------------+-----------------+---------------------------+---------------------------+--------------------+------------------+\n",
      "|WeekOfMonth|WeekOfMonthClaimed| Age|FraudFound_P|RepNumber|Deductible|DriverRating|  Year|AgeOfPolicyHolder_Min|AgeOfPolicyHolder_Max|New_Age|Month_indexed|DayOfWeek_indexed|Make_indexed|AccidentArea_indexed|DayOfWeekClaimed_indexed|MonthClaimed_indexed|Sex_indexed|MaritalStatus_indexed|Fault_indexed|PolicyType_indexed|VehicleCategory_indexed|VehiclePrice_indexed|Days_Policy_Accident_indexed|Days_Policy_Claim_indexed|PastNumberOfClaims_indexed|AgeOfVehicle_indexed|AgeOfPolicyHolder_indexed|PoliceReportFiled_indexed|WitnessPresent_indexed|AgentType_indexed|NumberOfSuppliments_indexed|AddressChange_Claim_indexed|NumberOfCars_indexed|BasePolicy_indexed| Month_encoded|DayOfWeek_encoded|  Make_encoded|AccidentArea_encoded|DayOfWeekClaimed_encoded|MonthClaimed_encoded|  Sex_encoded|MaritalStatus_encoded|Fault_encoded|PolicyType_encoded|VehicleCategory_encoded|VehiclePrice_encoded|Days_Policy_Accident_encoded|Days_Policy_Claim_encoded|PastNumberOfClaims_encoded|AgeOfVehicle_encoded|AgeOfPolicyHolder_encoded|PoliceReportFiled_encoded|WitnessPresent_encoded|AgentType_encoded|NumberOfSuppliments_encoded|AddressChange_Claim_encoded|NumberOfCars_encoded|BasePolicy_encoded|\n",
      "+-----------+------------------+----+------------+---------+----------+------------+------+---------------------+---------------------+-------+-------------+-----------------+------------+--------------------+------------------------+--------------------+-----------+---------------------+-------------+------------------+-----------------------+--------------------+----------------------------+-------------------------+--------------------------+--------------------+-------------------------+-------------------------+----------------------+-----------------+---------------------------+---------------------------+--------------------+------------------+--------------+-----------------+--------------+--------------------+------------------------+--------------------+-------------+---------------------+-------------+------------------+-----------------------+--------------------+----------------------------+-------------------------+--------------------------+--------------------+-------------------------+-------------------------+----------------------+-----------------+---------------------------+---------------------------+--------------------+------------------+\n",
      "|        1.0|               1.0|33.0|         0.0|     16.0|     400.0|         3.0|1996.0|                   31|                   35|   33.0|          4.0|              1.0|         2.0|                 1.0|                     0.0|                 4.0|        1.0|                  0.0|          1.0|               0.0|                    0.0|                 0.0|                         0.0|                      0.0|                       2.0|                 3.0|                      0.0|                      0.0|                   0.0|              0.0|                        0.0|                        0.0|                 0.0|               0.0|(12,[4],[1.0])|    (7,[1],[1.0])|(18,[2],[1.0])|       (2,[1],[1.0])|           (8,[0],[1.0])|      (13,[4],[1.0])|(2,[1],[1.0])|        (4,[0],[1.0])|(2,[1],[1.0])|     (9,[0],[1.0])|          (3,[0],[1.0])|       (6,[0],[1.0])|               (5,[0],[1.0])|            (4,[0],[1.0])|             (4,[2],[1.0])|       (8,[3],[1.0])|            (9,[0],[1.0])|            (2,[0],[1.0])|         (2,[0],[1.0])|    (2,[0],[1.0])|              (4,[0],[1.0])|              (5,[0],[1.0])|       (5,[0],[1.0])|     (3,[0],[1.0])|\n",
      "+-----------+------------------+----+------------+---------+----------+------------+------+---------------------+---------------------+-------+-------------+-----------------+------------+--------------------+------------------------+--------------------+-----------+---------------------+-------------+------------------+-----------------------+--------------------+----------------------------+-------------------------+--------------------------+--------------------+-------------------------+-------------------------+----------------------+-----------------+---------------------------+---------------------------+--------------------+------------------+--------------+-----------------+--------------+--------------------+------------------------+--------------------+-------------+---------------------+-------------+------------------+-----------------------+--------------------+----------------------------+-------------------------+--------------------------+--------------------+-------------------------+-------------------------+----------------------+-----------------+---------------------------+---------------------------+--------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Vehicle Insurance Claim Fraud Detection\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Load the dataset of vehicle insurance claim fraud\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../dataset/vehicle_insurance_claim_fraud_data.csv\")\n",
    "\n",
    "# Split the AgeOfPolicyHolder column into two columns, representing the minimum and maximum age values\n",
    "df = df.withColumn(\"AgeOfPolicyHolder_Min\", split(col(\"AgeOfPolicyHolder\"), \" \")[0].cast(\"int\"))\n",
    "df = df.withColumn(\"AgeOfPolicyHolder_Max\", split(col(\"AgeOfPolicyHolder\"), \" \")[2].cast(\"int\"))\n",
    "\n",
    "# Compute the average age range for each row in the AgeOfPolicyHolder column\n",
    "avg_age_range = df.select(avg((col(\"AgeOfPolicyHolder_Min\") + col(\"AgeOfPolicyHolder_Max\")) / 2)).collect()[0][0]\n",
    "\n",
    "# Add a new column that copies the Age column value when it is not 0, or the average age range otherwise\n",
    "df = df.withColumn(\"New_Age\", when(col(\"Age\") == 0, avg_age_range).otherwise(col(\"Age\")))\n",
    "\n",
    "# Convert the columns having numerical values to Double type\n",
    "numerical_cols = ['WeekOfMonth', 'WeekOfMonthClaimed', 'Age', 'New_Age', 'FraudFound_P', 'RepNumber', 'Deductible', 'DriverRating', 'Year']\n",
    "for col in numerical_cols:\n",
    "    df = df.withColumn(col, df[col].cast(DoubleType()))\n",
    "\n",
    "# Divide the dataset into training and test sets\n",
    "training, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Perform one-hot encoding for categorical features on the training set\n",
    "categorical_cols = ['Month', 'DayOfWeek', 'Make', 'AccidentArea', 'DayOfWeekClaimed',\n",
    "       'MonthClaimed', 'Sex', 'MaritalStatus', 'Fault', 'PolicyType',\n",
    "       'VehicleCategory', 'VehiclePrice', 'Days_Policy_Accident',\n",
    "       'Days_Policy_Claim', 'PastNumberOfClaims', 'AgeOfVehicle',\n",
    "       'AgeOfPolicyHolder', 'PoliceReportFiled', 'WitnessPresent', 'AgentType',\n",
    "       'NumberOfSuppliments', 'AddressChange_Claim', 'NumberOfCars',\n",
    "       'BasePolicy']\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_indexed\", handleInvalid=\"keep\") for col in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=col+\"_indexed\", outputCol=col+\"_encoded\") for col in categorical_cols]\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "pipelineModel = pipeline.fit(training)\n",
    "training_encoded = pipelineModel.transform(training).drop(*categorical_cols, *[\"PolicyNumber\"])\n",
    "test_encoded = pipelineModel.transform(test).drop(*categorical_cols, *[\"PolicyNumber\"])\n",
    "print('Data is processed.')\n",
    "training_encoded.show(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WeekOfMonth',\n",
       " 'WeekOfMonthClaimed',\n",
       " 'Age',\n",
       " 'FraudFound_P',\n",
       " 'RepNumber',\n",
       " 'Deductible',\n",
       " 'DriverRating',\n",
       " 'Year',\n",
       " 'AgeOfPolicyHolder_Min',\n",
       " 'AgeOfPolicyHolder_Max',\n",
       " 'New_Age',\n",
       " 'Month_indexed',\n",
       " 'DayOfWeek_indexed',\n",
       " 'Make_indexed',\n",
       " 'AccidentArea_indexed',\n",
       " 'DayOfWeekClaimed_indexed',\n",
       " 'MonthClaimed_indexed',\n",
       " 'Sex_indexed',\n",
       " 'MaritalStatus_indexed',\n",
       " 'Fault_indexed',\n",
       " 'PolicyType_indexed',\n",
       " 'VehicleCategory_indexed',\n",
       " 'VehiclePrice_indexed',\n",
       " 'Days_Policy_Accident_indexed',\n",
       " 'Days_Policy_Claim_indexed',\n",
       " 'PastNumberOfClaims_indexed',\n",
       " 'AgeOfVehicle_indexed',\n",
       " 'AgeOfPolicyHolder_indexed',\n",
       " 'PoliceReportFiled_indexed',\n",
       " 'WitnessPresent_indexed',\n",
       " 'AgentType_indexed',\n",
       " 'NumberOfSuppliments_indexed',\n",
       " 'AddressChange_Claim_indexed',\n",
       " 'NumberOfCars_indexed',\n",
       " 'BasePolicy_indexed',\n",
       " 'Month_encoded',\n",
       " 'DayOfWeek_encoded',\n",
       " 'Make_encoded',\n",
       " 'AccidentArea_encoded',\n",
       " 'DayOfWeekClaimed_encoded',\n",
       " 'MonthClaimed_encoded',\n",
       " 'Sex_encoded',\n",
       " 'MaritalStatus_encoded',\n",
       " 'Fault_encoded',\n",
       " 'PolicyType_encoded',\n",
       " 'VehicleCategory_encoded',\n",
       " 'VehiclePrice_encoded',\n",
       " 'Days_Policy_Accident_encoded',\n",
       " 'Days_Policy_Claim_encoded',\n",
       " 'PastNumberOfClaims_encoded',\n",
       " 'AgeOfVehicle_encoded',\n",
       " 'AgeOfPolicyHolder_encoded',\n",
       " 'PoliceReportFiled_encoded',\n",
       " 'WitnessPresent_encoded',\n",
       " 'AgentType_encoded',\n",
       " 'NumberOfSuppliments_encoded',\n",
       " 'AddressChange_Claim_encoded',\n",
       " 'NumberOfCars_encoded',\n",
       " 'BasePolicy_encoded']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_encoded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best maxDepth:  5\n",
      "Best minInstancesPerNode:  5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "features = ['WeekOfMonth',\n",
    " 'WeekOfMonthClaimed',\n",
    " 'Age',\n",
    " 'RepNumber',\n",
    " 'Deductible',\n",
    " 'DriverRating',\n",
    " 'Year',\n",
    " 'AgeOfPolicyHolder_Min',\n",
    " 'AgeOfPolicyHolder_Max',\n",
    " 'New_Age',\n",
    " 'Month_indexed',\n",
    " 'DayOfWeek_indexed',\n",
    " 'Make_indexed',\n",
    " 'AccidentArea_indexed',\n",
    " 'DayOfWeekClaimed_indexed',\n",
    " 'MonthClaimed_indexed',\n",
    " 'Sex_indexed',\n",
    " 'MaritalStatus_indexed',\n",
    " 'Fault_indexed',\n",
    " 'PolicyType_indexed',\n",
    " 'VehicleCategory_indexed',\n",
    " 'VehiclePrice_indexed',\n",
    " 'Days_Policy_Accident_indexed',\n",
    " 'Days_Policy_Claim_indexed',\n",
    " 'PastNumberOfClaims_indexed',\n",
    " 'AgeOfVehicle_indexed',\n",
    " 'AgeOfPolicyHolder_indexed',\n",
    " 'PoliceReportFiled_indexed',\n",
    " 'WitnessPresent_indexed',\n",
    " 'AgentType_indexed',\n",
    " 'NumberOfSuppliments_indexed',\n",
    " 'AddressChange_Claim_indexed',\n",
    " 'NumberOfCars_indexed',\n",
    " 'BasePolicy_indexed',\n",
    " 'Month_encoded',\n",
    " 'DayOfWeek_encoded',\n",
    " 'Make_encoded',\n",
    " 'AccidentArea_encoded',\n",
    " 'DayOfWeekClaimed_encoded',\n",
    " 'MonthClaimed_encoded',\n",
    " 'Sex_encoded',\n",
    " 'MaritalStatus_encoded',\n",
    " 'Fault_encoded',\n",
    " 'PolicyType_encoded',\n",
    " 'VehicleCategory_encoded',\n",
    " 'VehiclePrice_encoded',\n",
    " 'Days_Policy_Accident_encoded',\n",
    " 'Days_Policy_Claim_encoded',\n",
    " 'PastNumberOfClaims_encoded',\n",
    " 'AgeOfVehicle_encoded',\n",
    " 'AgeOfPolicyHolder_encoded',\n",
    " 'PoliceReportFiled_encoded',\n",
    " 'WitnessPresent_encoded',\n",
    " 'AgentType_encoded',\n",
    " 'NumberOfSuppliments_encoded',\n",
    " 'AddressChange_Claim_encoded',\n",
    " 'NumberOfCars_encoded',\n",
    " 'BasePolicy_encoded']\n",
    "\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "train_df = assembler.transform(training_encoded).select(\"features\", \"FraudFound_P\")\n",
    "\n",
    "# Define the decision tree classifier\n",
    "dt = DecisionTreeClassifier(labelCol=\"FraudFound_P\", featuresCol=\"features\")\n",
    "\n",
    "# Define the parameter grid for cross-validation\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [2, 5, 10]) \\\n",
    "    .addGrid(dt.minInstancesPerNode, [1, 5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "# Define the evaluator for binary classification\n",
    "#evaluator = BinaryClassificationEvaluator(labelCol=\"FraudFound_P\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"FraudFound_P\", \n",
    "                      metricName=\"weightedPrecision\")\n",
    "\n",
    "# Define the cross-validator with k-fold stratified sampling\n",
    "cv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=precision_evaluator, numFolds=5, seed=42)\n",
    "\n",
    "# Run the cross-validation on the dataset\n",
    "cvModel = cv.fit(train_df)\n",
    "\n",
    "# Print the best model's parameters\n",
    "print(\"Best maxDepth: \", cvModel.bestModel._java_obj.getMaxDepth())\n",
    "print(\"Best minInstancesPerNode: \", cvModel.bestModel._java_obj.getMinInstancesPerNode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9446570694569505\n",
      "0.9501916389138795\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('WeekOfMonth', 0.0),\n",
       " ('WeekOfMonthClaimed', 0.0),\n",
       " ('Age', 0.0),\n",
       " ('RepNumber', 0.0),\n",
       " ('Deductible', 0.0),\n",
       " ('DriverRating', 0.0),\n",
       " ('Year', 0.0),\n",
       " ('AgeOfPolicyHolder_Min', 0.0),\n",
       " ('AgeOfPolicyHolder_Max', 0.0),\n",
       " ('New_Age', 0.0),\n",
       " ('Month_indexed', 0.0),\n",
       " ('DayOfWeek_indexed', 0.0),\n",
       " ('Make_indexed', 0.0),\n",
       " ('AccidentArea_indexed', 0.0),\n",
       " ('DayOfWeekClaimed_indexed', 0.0),\n",
       " ('MonthClaimed_indexed', 0.0),\n",
       " ('Sex_indexed', 0.0),\n",
       " ('MaritalStatus_indexed', 0.0),\n",
       " ('Fault_indexed', 0.3540980997607375),\n",
       " ('PolicyType_indexed', 0.0),\n",
       " ('VehicleCategory_indexed', 0.0),\n",
       " ('VehiclePrice_indexed', 0.0),\n",
       " ('Days_Policy_Accident_indexed', 0.0),\n",
       " ('Days_Policy_Claim_indexed', 0.0),\n",
       " ('PastNumberOfClaims_indexed', 0.0),\n",
       " ('AgeOfVehicle_indexed', 0.0),\n",
       " ('AgeOfPolicyHolder_indexed', 0.1762011033110427),\n",
       " ('PoliceReportFiled_indexed', 0.0),\n",
       " ('WitnessPresent_indexed', 0.0),\n",
       " ('AgentType_indexed', 0.0),\n",
       " ('NumberOfSuppliments_indexed', 0.0),\n",
       " ('AddressChange_Claim_indexed', 0.21993456490337832),\n",
       " ('NumberOfCars_indexed', 0.0),\n",
       " ('BasePolicy_indexed', 0.2497662320248416),\n",
       " ('Month_encoded', 0.0),\n",
       " ('DayOfWeek_encoded', 0.0),\n",
       " ('Make_encoded', 0.0),\n",
       " ('AccidentArea_encoded', 0.0),\n",
       " ('DayOfWeekClaimed_encoded', 0.0),\n",
       " ('MonthClaimed_encoded', 0.0),\n",
       " ('Sex_encoded', 0.0),\n",
       " ('MaritalStatus_encoded', 0.0),\n",
       " ('Fault_encoded', 0.0),\n",
       " ('PolicyType_encoded', 0.0),\n",
       " ('VehicleCategory_encoded', 0.0),\n",
       " ('VehiclePrice_encoded', 0.0),\n",
       " ('Days_Policy_Accident_encoded', 0.0),\n",
       " ('Days_Policy_Claim_encoded', 0.0),\n",
       " ('PastNumberOfClaims_encoded', 0.0),\n",
       " ('AgeOfVehicle_encoded', 0.0),\n",
       " ('AgeOfPolicyHolder_encoded', 0.0),\n",
       " ('PoliceReportFiled_encoded', 0.0),\n",
       " ('WitnessPresent_encoded', 0.0),\n",
       " ('AgentType_encoded', 0.0),\n",
       " ('NumberOfSuppliments_encoded', 0.0),\n",
       " ('AddressChange_Claim_encoded', 0.0),\n",
       " ('NumberOfCars_encoded', 0.0),\n",
       " ('BasePolicy_encoded', 0.0)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_df = assembler.transform(training_encoded).select(\"features\", \"FraudFound_P\")\n",
    "test_df = assembler.transform(test_encoded).select(\"features\", \"FraudFound_P\")\n",
    "print(precision_evaluator.evaluate(cvModel.transform(train_df)))\n",
    "print(precision_evaluator.evaluate(cvModel.transform(test_df)))\n",
    "list(zip(features, cvModel.bestModel.featureImportances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shefali Upadhyaya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shefali Upadhyaya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\serializers.py\", line 458, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "  File \"C:\\Users\\Shefali Upadhyaya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 73, in dumps\n",
      "    cp.dump(obj)\n",
      "  File \"C:\\Users\\Shefali Upadhyaya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 602, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "TypeError: cannot pickle '_thread.RLock' object\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: TypeError: cannot pickle '_thread.RLock' object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\serializers.py:458\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcloudpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[0;32m     70\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[0;32m     71\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[0;32m     72\u001b[0m )\n\u001b[1;32m---> 73\u001b[0m \u001b[43mcp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:602\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 602\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot pickle '_thread.RLock' object",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m predictionAndLabels \u001b[38;5;241m=\u001b[39m test_encoded\u001b[38;5;241m.\u001b[39mrdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m lp: (\u001b[38;5;28mfloat\u001b[39m(cvModel\u001b[38;5;241m.\u001b[39mpredict(lp\u001b[38;5;241m.\u001b[39mfeatures)), lp\u001b[38;5;241m.\u001b[39mFraudFound_P))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Instantiate metrics object\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mMulticlassMetrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictionAndLabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Overall statistics\u001b[39;00m\n\u001b[0;32m      8\u001b[0m precision \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mprecision()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\mllib\\evaluation.py:288\u001b[0m, in \u001b[0;36mMulticlassMetrics.__init__\u001b[1;34m(self, predictionAndLabels)\u001b[0m\n\u001b[0;32m    286\u001b[0m sc \u001b[38;5;241m=\u001b[39m predictionAndLabels\u001b[38;5;241m.\u001b[39mctx\n\u001b[0;32m    287\u001b[0m sql_ctx \u001b[38;5;241m=\u001b[39m SQLContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sc)\n\u001b[1;32m--> 288\u001b[0m numCol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mpredictionAndLabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    289\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType(\n\u001b[0;32m    290\u001b[0m     [\n\u001b[0;32m    291\u001b[0m         StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m, DoubleType(), nullable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m    292\u001b[0m         StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, DoubleType(), nullable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m    293\u001b[0m     ]\n\u001b[0;32m    294\u001b[0m )\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numCol \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\rdd.py:1903\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   1891\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1892\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   1893\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1901\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   1902\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1903\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1904\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[0;32m   1905\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\rdd.py:1883\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1880\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1882\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 1883\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1885\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m   1886\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:1486\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1484\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   1485\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1486\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mrunJob(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc(), \u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m, partitions)\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\rdd.py:3505\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   3502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3503\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 3505\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prev_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd_deserializer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofiler\u001b[49m\n\u001b[0;32m   3507\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3509\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3510\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\n\u001b[0;32m   3511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier\n\u001b[0;32m   3512\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\rdd.py:3362\u001b[0m, in \u001b[0;36m_wrap_function\u001b[1;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[0;32m   3360\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3361\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[1;32m-> 3362\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_for_python_RDD\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3363\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonFunction(\n\u001b[0;32m   3365\u001b[0m     \u001b[38;5;28mbytearray\u001b[39m(pickled_command),\n\u001b[0;32m   3366\u001b[0m     env,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3371\u001b[0m     sc\u001b[38;5;241m.\u001b[39m_javaAccumulator,\n\u001b[0;32m   3372\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\rdd.py:3345\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[1;34m(sc, command)\u001b[0m\n\u001b[0;32m   3342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_for_python_RDD\u001b[39m(sc: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkContext\u001b[39m\u001b[38;5;124m\"\u001b[39m, command: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mbytes\u001b[39m, Any, Any, Any]:\n\u001b[0;32m   3343\u001b[0m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[0;32m   3344\u001b[0m     ser \u001b[38;5;241m=\u001b[39m CloudPickleSerializer()\n\u001b[1;32m-> 3345\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m \u001b[43mser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3346\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[0;32m   3348\u001b[0m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\serializers.py:468\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    466\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[0;32m    467\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m--> 468\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[1;31mPicklingError\u001b[0m: Could not serialize object: TypeError: cannot pickle '_thread.RLock' object"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "predictionAndLabels = test_encoded.rdd.map(lambda lp: (float(cvModel.predict(lp.features)), lp.FraudFound_P))\n",
    "# Instantiate metrics object\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "# Overall statistics\n",
    "precision = metrics.precision()\n",
    "recall = metrics.recall()\n",
    "f1Score = metrics.fMeasure()\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
