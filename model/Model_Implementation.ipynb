{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Insurance Claim Fraud Detection\n",
    "### The goal of this notebook is to:\n",
    "1. Predict whether insurance claim fraud can be mitigated by training the algorithms (decision tree, random forest and logistic regression).\n",
    "2. Calculate and compare the performance of the 3 algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Gathering\n",
    "\n",
    "For vehicle insurance claim fraud, a dataset is chosen from [Kaggle](https://www.kaggle.com/datasets/shivamb/vehicle-claim-fraud-detection) having 33 features and 15420 data entries. This dataset has several features that indicate the following:\n",
    "\n",
    "| **Type** | **Features** |\n",
    "|:---:|:---:|\n",
    "| Vehicle | make, category, price, age |\n",
    "| Customer | sex, age, marital-status, driver-rating, number-of-vehicles |\n",
    "| Policy | policy-number, fault, type, deductible, days between events, police-report filed, witness present, agent-type, accident-area, address-change, base-policy, supplements |\n",
    "| Data labeled with binary classification if claim was fraudulent or not (1 or 0) | fraud-found |\n",
    "\n",
    "\n",
    "The data size available for this use-case is sufficient - more than 10 times degrees of freedom. The other observations ensure acceptable nulls/missing values (2%), the data is imbalanced 6%, however, it is in line with insurance industry trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session is initialized.\n",
      "Spark dataframe is created from the dataset.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initializing Spark session\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Vehicle Insurance Claim Fraud Detection\")\\\n",
    "        .getOrCreate()\n",
    "print('Spark session is initialized.')\n",
    "\n",
    "# Load the vehicle insurance claim fraud dataset from the csv file and create a dataframe\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../dataset/vehicle_insurance_claim_fraud_data.csv\")\n",
    "print('Spark dataframe is created from the dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "For vehicle insurance claim fraud, a dataset is chosen from Kaggle having 33 features and 15420 data entries. This dataset has several features that indicate the following:\n",
    "\n",
    "| **Type** | **Features** |\n",
    "|:---:|:---:|\n",
    "| Vehicle | make, category, price, age |\n",
    "| Customer | sex, age, marital-status, driver-rating, number-of-vehicles |\n",
    "| Policy | policy-number, fault, type, deductible, days between events, police-report filed, witness present, agent-type, accident-area, address-change, base-policy, supplements |\n",
    "| Data labeled with binary classification if claim was fraudulent or not (1 or 0) | fraud-found |\n",
    "\n",
    "\n",
    "The data size available for this use-case is sufficient - more than 10 times degrees of freedom. The other observations ensure acceptable nulls/missing values (2.07%), the data is imbalanced 6%, however, it is in line with insurance industry trends.\n",
    "\n",
    "For data preparation, following steps were performed to improve the quality of the data:\n",
    "1. Age: column has '0' as value for 2.07% rows.\n",
    "2. To impute 'Age' column having 0 as value, a new column 'Imputed_Age' has been added to the dataframe which computes the average age from the 'AgeOfPolicyHolder' column ranged value.\n",
    "3. All the numerical features are converted to double type to maintain uniformity.\n",
    "4. The dataset is split into training and test set using random split (80/20 ratio).\n",
    "5. One-hot encoding is performed for categorical features.\n",
    "6. Features are extracted and training and test sets are assembled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is prepared.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Adds a new column to the dataframe called 'Imputed_Age' that copies the 'Age' column value when it is not 0, \n",
    "# or the average age from the range given in 'AgeOfPolicyHolder' column value\n",
    "df = df.withColumn(\"Imputed_Age\", when(col(\"Age\") == 0, (split(col(\"AgeOfPolicyHolder\"), \" \")[0].cast(\"int\") + \n",
    "    split(col(\"AgeOfPolicyHolder\"), \" \")[0].cast(\"int\")) / 2).otherwise(col(\"Age\")))\n",
    "\n",
    "# Convert the columns having numerical values to Double type\n",
    "numerical_cols = ['WeekOfMonth', 'WeekOfMonthClaimed', 'Age', 'Imputed_Age', 'FraudFound_P', 'RepNumber', 'Deductible', \n",
    "                  'DriverRating', 'Year']\n",
    "for col in numerical_cols:\n",
    "    df = df.withColumn(col, df[col].cast(DoubleType()))\n",
    "\n",
    "# Divide the dataset into training and test sets\n",
    "training, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Create a list of the columns having categorical values\n",
    "categorical_cols = ['Month', 'DayOfWeek', 'Make', 'AccidentArea', 'DayOfWeekClaimed', 'MonthClaimed', 'Sex', 'MaritalStatus', \n",
    "                    'Fault', 'PolicyType', 'VehicleCategory', 'VehiclePrice', 'Days_Policy_Accident', 'Days_Policy_Claim', \n",
    "                    'PastNumberOfClaims', 'AgeOfVehicle', 'AgeOfPolicyHolder', 'PoliceReportFiled', 'WitnessPresent', \n",
    "                    'AgentType', 'NumberOfSuppliments', 'AddressChange_Claim', 'NumberOfCars', 'BasePolicy']\n",
    "\n",
    "# Perform one-hot encoding for categorical features\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_indexed\", handleInvalid=\"keep\") for col in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=col+\"_indexed\", outputCol=col+\"_encoded\") for col in categorical_cols]\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "pipelineModel = pipeline.fit(training)\n",
    "training_encoded = pipelineModel.transform(training).drop(*categorical_cols, *[\"PolicyNumber\"])\n",
    "test_encoded = pipelineModel.transform(test).drop(*categorical_cols, *[\"PolicyNumber\"])\n",
    "\n",
    "# Extracting the features and removing the 'FraudFound_P' column\n",
    "features = training_encoded.columns\n",
    "features.remove('FraudFound_P')\n",
    "\n",
    "# Assembles the columns into a feature vector for the training and test sets\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "train_df = assembler.transform(training_encoded).select(\"features\", \"FraudFound_P\")\n",
    "test_df = assembler.transform(test_encoded).select(\"features\", \"FraudFound_P\")\n",
    "\n",
    "# Count the number of records with FraudFound_P value as 0 and 1 in the training set\n",
    "count_0 = train_df.filter(train_df[\"FraudFound_P\"] == 0).count()\n",
    "count_1 = train_df.filter(train_df[\"FraudFound_P\"] == 1).count()\n",
    "\n",
    "# Calculate the ratio of FraudFound_P values 0 and 1\n",
    "fraud_ratio = count_1 / count_0\n",
    "\n",
    "# Create separate DataFrames for minority and majority class\n",
    "minority_df = train_df.filter(train_df[\"FraudFound_P\"] == 1)\n",
    "majority_df = train_df.filter(train_df[\"FraudFound_P\"] == 0)\n",
    "\n",
    "# Sample the majority class DataFrame to balance the dataset\n",
    "balanced_majority_df = majority_df.sample(fraction=fraud_ratio, seed=42)\n",
    "\n",
    "# Combine the minority class DataFrame and the balanced majority class DataFrame\n",
    "balanced_train_df = minority_df.unionAll(balanced_majority_df)\n",
    "\n",
    "# Shuffle the rows of the balanced DataFrame\n",
    "balanced_train_df = balanced_train_df.orderBy(rand())\n",
    "\n",
    "print('Data is prepared.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "For this usecase, model understandability is a major aspect for competitive markets such as insurance. Hence, the model were chosen in a way that they are whitebox, simple to understand and able to generate workable insights for upselling in the business. Example: If the model shows that individuals who has a voluntary deductible of \\$1000 don't commit fraud, customers willing to pay the same can be awarded with better insurance premiums. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Decision Tree Classifier\n",
    "Decision trees are the simplest and easiest to understand. To prevent overfitting, the model was adjusted by using techniques such as stratified sampling and balanced sampling. Fitting was performed on multiple hyperparameters settings: maxDepth, minInstancesPerNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Classifier, ParamGrid and Precision Evaluator is defined.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# Define the decision tree classifier\n",
    "dt = DecisionTreeClassifier(labelCol=\"FraudFound_P\", featuresCol=\"features\")\n",
    "\n",
    "# Define the parameter grid for cross-validation\n",
    "paramGrid_dt = ParamGridBuilder() \\\n",
    "    .addGrid(dt1.maxDepth, [2, 5, 10]) \\\n",
    "    .addGrid(dt1.minInstancesPerNode, [1, 5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "# Define the evaluator as precision for the classification\n",
    "evaluator_dt = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"FraudFound_P\",\n",
    "                                              metricName=\"weightedPrecision\")\n",
    "\n",
    "print('Decision Tree Classifier, ParamGrid and Precision Evaluator is defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Data Sampling\n",
    "1. Simple Random Sampling\n",
    "2. Stratified Sampling\n",
    "3. Balanced Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from stratifier import StratifiedCrossValidator\n",
    "\n",
    "# Define the k-fold cross-validator\n",
    "cv_dt = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid_dt, evaluator=evaluator_dt, numFolds=5)\n",
    "\n",
    "# Define the stratified k-fold cross-validator\n",
    "scv_dt = StratifiedCrossValidator(estimator=dt, estimatorParamMaps=paramGrid_dt, evaluator=evaluator_dt, numFolds=5)\n",
    "\n",
    "# Run the Decision Tree model on the training set\n",
    "random_model_dt = cv_dt.fit(train_df)\n",
    "\n",
    "# Run the Decision Tree model using stratified k-fold cross-validation on the training set\n",
    "stratified_model_dt = scv_dt2.fit(train_df)\n",
    "\n",
    "# Run the Decision Tree model on the balanced training set\n",
    "balanced_model_dt = cv_dt.fit(balanced_train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Decision Tree Model Comparison\n",
    "\n",
    "Following comparison metrics were used for model evaluation:\n",
    "1. Precision (since it’s more important to predict fraudsters)\n",
    "2. Recall\n",
    "3. F1 Score\n",
    "4. Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics are evaluated.\n"
     ]
    }
   ],
   "source": [
    "# Get the predictions on the test set from Random Sampling\n",
    "predictions_random_model = random_model_dt.transform(test_df)\n",
    "\n",
    "# Get the predictions on the test set from Stratified Sampling\n",
    "predictions_stratified_model = stratified_model_dt.transform(test_df)\n",
    "\n",
    "# Get the predictions on the test set from Balanced Sampling\n",
    "predictions_balanced_model = balanced_model_dt.transform(test_df)\n",
    "\n",
    "# Set the evaluator metric to Precision\n",
    "evaluator_dt.setMetricName('weightedPrecision')\n",
    "\n",
    "# Precision for the decision tree model on Random Sampling\n",
    "precision_random_model = evaluator_dt.evaluate(predictions_random_model)\n",
    "\n",
    "# Precision for the decision tree model on Stratified Sampling\n",
    "precision_stratified_model = evaluator_dt.evaluate(predictions_stratified_model)\n",
    "\n",
    "# Precision for the decision tree model on Balanced Sampling\n",
    "precision_balanced_model = evaluator_dt.evaluate(predictions_balanced_model)\n",
    "\n",
    "# Set the evaluator metric to Recall\n",
    "evaluator_dt.setMetricName('weightedRecall')\n",
    "\n",
    "# Recall for the decision tree model on Random Sampling\n",
    "recall_random_model = evaluator_dt.evaluate(predictions_random_model)\n",
    "\n",
    "# Recall for the decision tree model on Stratified Sampling\n",
    "recall_stratified_model = evaluator_dt.evaluate(predictions_stratified_model)\n",
    "\n",
    "# Recall for the decision tree model on Balanced Sampling\n",
    "recall_balanced_model = evaluator_dt.evaluate(predictions_balanced_model)\n",
    "\n",
    "# Set the evaluator metric to F1\n",
    "evaluator_dt.setMetricName('f1')\n",
    "\n",
    "# F1 Score for the decision tree model on Random Sampling\n",
    "f1_score_random_model = evaluator_dt.evaluate(predictions_random_model)\n",
    "\n",
    "# F1 Score for the decision tree model on Stratified Sampling\n",
    "f1_score_stratified_model = evaluator_dt.evaluate(predictions_stratified_model)\n",
    "\n",
    "# F1 Score for the decision tree model on Balanced Sampling\n",
    "f1_score_balanced_model = evaluator_dt.evaluate(predictions_balanced_model)\n",
    "\n",
    "# Set the evaluator metric to Accuracy\n",
    "evaluator_dt.setMetricName('accuracy')\n",
    "\n",
    "# Recall for the decision tree model on Random Sampling\n",
    "accuracy_random_model = evaluator_dt.evaluate(predictions_random_model)\n",
    "\n",
    "# Recall for the decision tree model on Stratified Sampling\n",
    "accuracy_stratified_model = evaluator_dt.evaluate(predictions_stratified_model)\n",
    "\n",
    "# Recall for the decision tree model on Balanced Sampling\n",
    "accuracy_balanced_model = evaluator_dt.evaluate(predictions_balanced_model)\n",
    "\n",
    "print('Metrics are evaluated.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| **Type of sampling** | **Precision** | **Recall** | **F1 Score** | **Accuracy** |\n",
       "|:---:|:---:|:---:|:---:|:---:|\n",
       "| Random | 0.94 | 0.95 | 0.92 | 0.95 |\n",
       "| Stratified | 0.94 | 0.95 | 0.92 | 0.95 |\n",
       "| Balanced | 0.95 | 0.59 | 0.70 | 0.59 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "table = \"| **Type of sampling** | **Precision** | **Recall** | **F1 Score** | **Accuracy** |\\n\" + \\\n",
    "                 \"|:---:|:---:|:---:|:---:|:---:|\\n\" + \\\n",
    "                 \"| Random | {:.2f} | {:.2f} | {:.2f} | {:.2f} |\\n\" + \\\n",
    "                 \"| Stratified | {:.2f} | {:.2f} | {:.2f} | {:.2f} |\\n\" + \\\n",
    "                 \"| Balanced | {:.2f} | {:.2f} | {:.2f} | {:.2f} |\"\n",
    "\n",
    "display(Markdown(table.format(precision_random_model, recall_random_model, \n",
    "                 f1_score_random_model, accuracy_random_model, precision_stratified_model, recall_stratified_model,\n",
    "                 f1_score_stratified_model, accuracy_stratified_model, precision_balanced_model, recall_balanced_model,\n",
    "                 f1_score_balanced_model, accuracy_balanced_model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Random Forest Classifier\n",
    "\n",
    "Random forest model was adjusted by using techniques such as stratified sampling and balanced sampling. Fitting was performed on multiple hyperparameters settings: numTrees, maxDepth, minInstancesPerNode, minInfoGain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Define the random forest classifier\n",
    "rf = RandomForestClassifier(labelCol=\"FraudFound_P\", featuresCol=\"features\")\n",
    "print('Random forest is defined.')\n",
    "\n",
    "# Define the parameter grid for cross-validation\n",
    "paramGrid_rf = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [50, 100]) \\\n",
    "    .addGrid(rf.maxDepth, [2, 5, 10]) \\\n",
    "    .addGrid(rf.minInstancesPerNode, [1, 5, 10]) \\\n",
    "    .addGrid(rf.minInfoGain, [0.0, 0.05]) \\\n",
    "    .build()\n",
    "print('Parameter grid for cross-validation is defined.')\n",
    "\n",
    "# Define the evaluator as precision for the classification\n",
    "evaluator_rf1 = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"FraudFound_P\", \n",
    "                      metricName=\"weightedPrecision\")\n",
    "print('Precision evaluator is defined.')\n",
    "\n",
    "cv_rf1 = CrossValidator(estimator=rf1, estimatorParamMaps=paramGrid_rf1, evaluator=evaluator_rf1, numFolds=5, seed=42)\n",
    "cvModel_rf1 = cv_rf.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Data Sampling\n",
    "1. Simple Random Sampling\n",
    "2. Stratified Sampling\n",
    "3. Balanced Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance of the model is evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with Random Forest Classifier and stratified k-fold cross validation\n",
    "\n",
    "The model is trained with random forest classifier on the training dataset with precision as evaluator and stratified k-fold cross validation is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The important features are obtained from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance of the model is evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with Random Forest Classifier and stratified k-fold cross validation using balanced dataset\n",
    "\n",
    "The model is trained with random forest classifier on the training dataset with precision as evaluator and stratified k-fold cross validation is used on balanced dataset by undersampling the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The important features are obtained from the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance of the model is evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model with Logistic Regression and k-fold cross validation\n",
    "\n",
    "The model is trained with logistic regression on the training dataset with precision as evaluator and k-fold cross validation is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"FraudFound_P\", featuresCol=\"features\",maxIter = 10)\n",
    "\n",
    "import numpy as np\n",
    "paramGrid_lr = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, np.linspace(0.3, 0.01, 10)) \\\n",
    "    .addGrid(lr.elasticNetParam, np.linspace(0.3, 0.8, 6)) \\\n",
    "    .build()\n",
    "\n",
    "# Define the evaluator as precision for the classification\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"FraudFound_P\")\n",
    "print('Precision evaluator is defined.')\n",
    "\n",
    "crossval_lr = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid_lr,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds= 5)  \n",
    "cvModel_lr = crossval_lr.fit(train_df)\n",
    "predictions = cvModel_lr.transform(test_df)\n",
    "metricValue = evaluator.evaluate(predictions)\n",
    "print(\"Metric value = %s\" % metricValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The important features are obtained from the trained model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
