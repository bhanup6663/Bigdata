{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modules are imported.\n"
     ]
    }
   ],
   "source": [
    "# Importing required modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "print('Modules are imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session is initialized.\n"
     ]
    }
   ],
   "source": [
    "# Initializing Spark session\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Vehicle Insurance Claim Fraud Detection\")\\\n",
    "        .getOrCreate()\n",
    "print('Spark session is initialized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark dataframe is created.\n"
     ]
    }
   ],
   "source": [
    "# Load the vehicle insurance claim fraud dataset from the csv file and create a dataframe\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../dataset/vehicle_insurance_claim_fraud_data.csv\")\n",
    "print('Spark dataframe is created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Imputed_Age' column is added to the Spark dataframe.\n"
     ]
    }
   ],
   "source": [
    "# Adds a new column to the dataframe called 'Imputed_Age' that copies the 'Age' column value when it is not 0, \n",
    "# or the average age from the range given in 'AgeOfPolicyHolder' column value\n",
    "df = df.withColumn(\"Imputed_Age\", when(col(\"Age\") == 0, (split(col(\"AgeOfPolicyHolder\"), \" \")[0].cast(\"int\") + \n",
    "    split(col(\"AgeOfPolicyHolder\"), \" \")[0].cast(\"int\")) / 2).otherwise(col(\"Age\")))\n",
    "print(\"'Imputed_Age' column is added to the Spark dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns having numerical values to Double type.\n"
     ]
    }
   ],
   "source": [
    "# Convert the columns having numerical values to Double type\n",
    "numerical_cols = ['WeekOfMonth', 'WeekOfMonthClaimed', 'Age', 'Imputed_Age', 'FraudFound_P', 'RepNumber', 'Deductible', \n",
    "                  'DriverRating', 'Year']\n",
    "for col in numerical_cols:\n",
    "    df = df.withColumn(col, df[col].cast(DoubleType()))\n",
    "    \n",
    "print('Columns having numerical values to Double type.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is divided into training and test sets with random split.\n"
     ]
    }
   ],
   "source": [
    "# Divide the dataset into training and test sets\n",
    "training, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "print('Dataset is divided into training and test sets with random split.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A list of the columns having categorical values is created.\n"
     ]
    }
   ],
   "source": [
    "# Create a list of the columns having categorical values\n",
    "categorical_cols = ['Month', 'DayOfWeek', 'Make', 'AccidentArea', 'DayOfWeekClaimed', 'MonthClaimed', 'Sex', 'MaritalStatus', \n",
    "                    'Fault', 'PolicyType', 'VehicleCategory', 'VehiclePrice', 'Days_Policy_Accident', 'Days_Policy_Claim', \n",
    "                    'PastNumberOfClaims', 'AgeOfVehicle', 'AgeOfPolicyHolder', 'PoliceReportFiled', 'WitnessPresent', \n",
    "                    'AgentType', 'NumberOfSuppliments', 'AddressChange_Claim', 'NumberOfCars', 'BasePolicy']\n",
    "print('A list of the columns having categorical values is created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding is performed for the categorical features on the training set.\n"
     ]
    }
   ],
   "source": [
    "# Perform one-hot encoding for categorical features on the training set\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_indexed\", handleInvalid=\"keep\") for col in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=col+\"_indexed\", outputCol=col+\"_encoded\") for col in categorical_cols]\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "pipelineModel = pipeline.fit(training)\n",
    "training_encoded = pipelineModel.transform(training).drop(*categorical_cols, *[\"PolicyNumber\"])\n",
    "test_encoded = pipelineModel.transform(test).drop(*categorical_cols, *[\"PolicyNumber\"])\n",
    "print('One-hot encoding is performed for the categorical features on the training set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Features' are extracted.\n"
     ]
    }
   ],
   "source": [
    "# Extracting the features and removing the 'FraudFound_P' column\n",
    "features = training_encoded.columns\n",
    "features.remove('FraudFound_P')\n",
    "print(\"'Features' are extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and test sets are assembled.\n"
     ]
    }
   ],
   "source": [
    "# Assembles the columns into a feature vector for the training and test sets\n",
    "assembler = VectorAssembler(inputCols=features, outputCol=\"features\", handleInvalid=\"keep\")\n",
    "train_df = assembler.transform(training_encoded).select(\"features\", \"FraudFound_P\")\n",
    "test_df = assembler.transform(test_encoded).select(\"features\", \"FraudFound_P\")\n",
    "print('Training and test sets are assembled.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree is defined.\n"
     ]
    }
   ],
   "source": [
    "# Define the decision tree classifier\n",
    "dt = DecisionTreeClassifier(labelCol=\"FraudFound_P\", featuresCol=\"features\")\n",
    "print('Decision tree is defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter grid for cross-validation is defined.\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for cross-validation\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(dt.maxDepth, [2, 5, 10]) \\\n",
    "    .addGrid(dt.minInstancesPerNode, [1, 5, 10]) \\\n",
    "    .build()\n",
    "print('Parameter grid for cross-validation is defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision evaluator is defined.\n"
     ]
    }
   ],
   "source": [
    "# Define the evaluator as precision for the classification\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"FraudFound_P\", \n",
    "                      metricName=\"weightedPrecision\")\n",
    "print('Precision evaluator is defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validator with k-fold stratified sampling is defined.\n"
     ]
    }
   ],
   "source": [
    "# Define the cross-validator with k-fold stratified sampling\n",
    "cv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5, seed=42)\n",
    "print('Cross-validator with k-fold stratified sampling is defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified k-fold cross-validation is executed on the training set.\n"
     ]
    }
   ],
   "source": [
    "# Run the cross-validation on the training set\n",
    "cvModel = cv.fit(train_df)\n",
    "print('Stratified k-fold cross-validation is executed on the training set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best maxDepth:  5\n",
      "Best minInstancesPerNode:  5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('WeekOfMonth', 0.0),\n",
       " ('WeekOfMonthClaimed', 0.0),\n",
       " ('Age', 0.0),\n",
       " ('RepNumber', 0.0),\n",
       " ('Deductible', 0.0),\n",
       " ('DriverRating', 0.0),\n",
       " ('Year', 0.0),\n",
       " ('Imputed_Age', 0.0),\n",
       " ('Month_indexed', 0.0),\n",
       " ('DayOfWeek_indexed', 0.0),\n",
       " ('Make_indexed', 0.0),\n",
       " ('AccidentArea_indexed', 0.0),\n",
       " ('DayOfWeekClaimed_indexed', 0.0),\n",
       " ('MonthClaimed_indexed', 0.0),\n",
       " ('Sex_indexed', 0.0),\n",
       " ('MaritalStatus_indexed', 0.0),\n",
       " ('Fault_indexed', 0.3540980997607374),\n",
       " ('PolicyType_indexed', 0.0),\n",
       " ('VehicleCategory_indexed', 0.0),\n",
       " ('VehiclePrice_indexed', 0.0),\n",
       " ('Days_Policy_Accident_indexed', 0.0),\n",
       " ('Days_Policy_Claim_indexed', 0.0),\n",
       " ('PastNumberOfClaims_indexed', 0.0),\n",
       " ('AgeOfVehicle_indexed', 0.0),\n",
       " ('AgeOfPolicyHolder_indexed', 0.17620110331104266),\n",
       " ('PoliceReportFiled_indexed', 0.0),\n",
       " ('WitnessPresent_indexed', 0.0),\n",
       " ('AgentType_indexed', 0.0),\n",
       " ('NumberOfSuppliments_indexed', 0.0),\n",
       " ('AddressChange_Claim_indexed', 0.2199345649033783),\n",
       " ('NumberOfCars_indexed', 0.0),\n",
       " ('BasePolicy_indexed', 0.24976623202484158),\n",
       " ('Month_encoded', 0.0),\n",
       " ('DayOfWeek_encoded', 0.0),\n",
       " ('Make_encoded', 0.0),\n",
       " ('AccidentArea_encoded', 0.0),\n",
       " ('DayOfWeekClaimed_encoded', 0.0),\n",
       " ('MonthClaimed_encoded', 0.0),\n",
       " ('Sex_encoded', 0.0),\n",
       " ('MaritalStatus_encoded', 0.0),\n",
       " ('Fault_encoded', 0.0),\n",
       " ('PolicyType_encoded', 0.0),\n",
       " ('VehicleCategory_encoded', 0.0),\n",
       " ('VehiclePrice_encoded', 0.0),\n",
       " ('Days_Policy_Accident_encoded', 0.0),\n",
       " ('Days_Policy_Claim_encoded', 0.0),\n",
       " ('PastNumberOfClaims_encoded', 0.0),\n",
       " ('AgeOfVehicle_encoded', 0.0),\n",
       " ('AgeOfPolicyHolder_encoded', 0.0),\n",
       " ('PoliceReportFiled_encoded', 0.0),\n",
       " ('WitnessPresent_encoded', 0.0),\n",
       " ('AgentType_encoded', 0.0),\n",
       " ('NumberOfSuppliments_encoded', 0.0),\n",
       " ('AddressChange_Claim_encoded', 0.0),\n",
       " ('NumberOfCars_encoded', 0.0),\n",
       " ('BasePolicy_encoded', 0.0)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the best model's parameters\n",
    "print(\"Best maxDepth: \", cvModel.bestModel._java_obj.getMaxDepth())\n",
    "print(\"Best minInstancesPerNode: \", cvModel.bestModel._java_obj.getMinInstancesPerNode())\n",
    "list(zip(features, cvModel.bestModel.featureImportances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on the test set are obtained.\n"
     ]
    }
   ],
   "source": [
    "# Get the predictions on the test set\n",
    "predictions = cvModel.transform(test_df)\n",
    "print('Predictions on the test set are obtained.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.9501916389138795\n"
     ]
    }
   ],
   "source": [
    "# Precision metric\n",
    "precision = evaluator.evaluate(predictions)\n",
    "print(\"Precision = %s\" % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall = 0.9474206349206349\n"
     ]
    }
   ],
   "source": [
    "# Recall metric\n",
    "evaluator.setMetricName('weightedRecall')\n",
    "recall = evaluator.evaluate(predictions)\n",
    "print(\"Recall = %s\" % recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score = 0.9239699406348035\n"
     ]
    }
   ],
   "source": [
    "# F1 score metric\n",
    "evaluator.setMetricName('f1')\n",
    "f1_score = evaluator.evaluate(predictions)\n",
    "print(\"F1 Score = %s\" % f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9474206349206349\n"
     ]
    }
   ],
   "source": [
    "# Accuracy metric\n",
    "evaluator.setMetricName('accuracy')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %s\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
